```{r setup, include=FALSE}

knitr::opts_chunk$set(cache=FALSE)
knitr::opts_chunk$set(echo=FALSE)
knitr::opts_chunk$set(include=TRUE)
knitr::opts_chunk$set(results="asis")
knitr::opts_chunk$set(fig.width=12)
knitr::opts_chunk$set(fig.height=14)
prjpath <- "~/work/learn/competitions/kaggle/springleaf/"
datapath <- paste(prjpath, "data/", sep="")
analpath <- paste(prjpath, "analysis/", sep="")
rcodepath <- paste(analpath, "Rcode/", sep="")
setwd(analpath)

```


```{r loadLibsAndSource, include=FALSE}

reqdpkgs <- c("lattice", "latticeExtra", "ggplot2", "reshape2",
              "ggmap",	"plyr",  "lubridate",
              "Hmisc", "corrplot")
lapply(reqdpkgs, library, character.only=TRUE)

```

#Introduction



We study the correlations between the variables. We can use the
error-code fixed data, reading it directly fromt he saved csvs, or we
can use the original data (labelled of course), and use a list of
error-codes in each column when correlations are evaluated. Because we
might have a use for the error-code values, (for example we might want
to  create new variables ), we will follow the second plan. Values
representing error codes in the numerical variables of the data are 1
to 2 orders of magnitude larger than the normal values in that
variable. This makes sense if these large (or sometimes negative)
values are to represent errors. However, their large values will skew
any numerical study that does not notice them as error-codes. We have
identified the error-codes, and stored them in a list saved as Rdata.
We will use this list in this analysis.

```{r dataload}

train <- read.csv(paste(datapath, 'train_ecfixed.csv', sep=""),
                  stringsAsFactor=FALSE)
test <- read.csv(paste(datapath, 'test_ecfixed.csv', sep=""),
                 stringsAsFactor=FALSE)

```




There are two variables, *numeric_204, numeric_476*, that do not have
any data in the training set. We should remove this from our data,


```{r datatypes}

N.original <- nrow(train)
V <- ncol(train)
predictors <- names(train)[-V]
predictor_types <- sapply(predictors, function(p) class(train[, p]))
print("Data types in the data frame train")
print(table(predictor_types))

```
We will separate the columns by their data-type,

```{r colByDataType}

predictors_chr <- predictors[predictor_types == 'character']
predictors_num <- predictors[predictor_types == 'integer' |
                                 predictor_types == 'numeric']


predictors_chr <- predictors[predictor_types == 'character']
predictors_num <- predictors[predictor_types == 'integer' |
                                 predictor_types == 'numeric']

```
```{r assumedvalsfew}

predictor.values.counts <- function(p, data=train){
    vals <- sort(unique(train[, p]), na.last=TRUE)
    tab <- data.frame(table(train[, p], useNA='ifany'))
    names(tab) <- c('value', 'count')
    tab$predictor <- p
    tab[, c("predictor", "value", "count")]
}

predictor.values <- function(vars, n, data=train) {
    mat <- cbind(vars,
                 do.call(rbind,
                         lapply(vars,
                                function(p) {
                                    sort(unique(train[, p]), na.last=TRUE)
                                })
                         )
                 )
    df <- data.frame(mat, stringsAsFactors=FALSE)
    names(df) <- c("variable", paste("value", 1:n, sep="."))
    df$entropy <- sapply(vars,
                         function(p) {
                             tab <- as.numeric(table(train[, p]))
                             prob <- tab/sum(tab)
                             -1 * sum(prob * log(prob))
                         })
    df$num.eff.vars <- exp(df$entropy)
    df
}

predictor.entropies <- function(vars, data=train, useNA='no') {
    df <- data.frame(
        variable = vars,
        entropy = sapply(vars, function(p) {
                             tab <- as.numeric(table(train[,p]), useNA = useNA)
                             p <- tab/sum(tab)
                             - sum(p * log2(p))
                         }),
        median = sapply(vars, function(p) {
                            median(train[,p], na.rm=TRUE)
                        }),
        max = sapply(vars, function(p) {
                         max(train[,p], na.rm=TRUE)
                     }),
        min = sapply(vars, function(p) {
                         min(train[, p], na.rm=TRUE)
                     }),
        distinct_vals = sapply(vars, function(p) {
                                  length(unique(train[,p], na.rm=TRUE))
                              })
        )

    df$num_eff_vals <- 2**(df$entropy)
    df
}

```

We have written a function that computes the entropies of the
variables. We use that function to compute the entropies for the
error-code fixed data. We expect the function to be loaded in this R session.

```{r varentropies}

var.entropies <- predictor.entropies(names(train))
var.entropies$median <- as.numeric(var.entropies$median)
var.entropies$max <- as.numeric(var.entropies$max)
var.entropies$min <- as.numeric(var.entropies$min)
var.entropies$datatype <- predictor_types[var.entropies$variable]

```

**NOTICE** We have labelled the data variables as *date_xxx*, but have
  not labelled them as such above. These variables should be labelled
  as _character_, and we will relabel them as _date_ when
  appropriate. Another thing to remember is that the numeric variables
  are both _integer_ and _numeric_ in _var.entropies$datatype_.


#Highly correlated variables.

## Columns with no data

```{r removeNoDataColumns}

print(subset(var.entropies, nacount==nrow(train)))
print(paste("number of columns in train", ncol(train)))
vars_nodata <- with(var.entropies, variable[nacount==N.original])
train <- train[, !(names(train) %in% vars_nodata)]
test <- test[, !(names(train) %in% vars_nodata)]
print(paste("after removing columns without data", ncol(train)))

# Highly correlated variables.


We should expect high correlations only between variables that live in
the same phase space, so the highly correlated variabels should assume
the same set of values, and thus the same number of distinct values
As starters, lets consider numerical variables with 3 or 4 distinct
vals. Since correlations are defined only for numerical variables, we
will subset the variables accordingly.

```{r threevalcor}

var.3val.cor <- cor(train[, with(var.entropies,
                                 variable[distinct_vals == 3 &
                                              (datatype == 'integer' |
                                                   datatype == 'numeric')

                                          ]
                                 )
                          ],
                    use='pairwise.complete.obs'
                    )
var.3val.cor[is.na(var.3val.cor)] <- 0
corrplot(var.3val.cor, method='color', tl.cex=0.5)

```

```{r threevalcor}

var.4val.cor <- cor(train[, with(var.entropies,
                                 variable[distinct_vals == 4 &
                                              (datatype == 'integer' |
                                                   datatype == 'numeric')

                                          ]
                                 )
                          ],
                    use='pairwise.complete.obs'
                    )
var.4val.cor[is.na(var.4val.cor)] <- 0
corrplot(var.4val.cor, method='color', tl.cex=0.5)

```

```{r cortrulynumeric}

var_truly_numeric <- with(var.entropies,
                          variable[datatype == 'numeric' &
                                       !is.na(datatype)])
numcors <- cor(train[, var_truly_numeric],
               use='pairwise.complete.obs'
               )
numcors[is.na(numcors)] <- 0

corrplot(numcors, order='AOE')

```

```{r allcors.nona}

vars.num.nona <- with(var.entropies,
                      variable[(datatype == 'integer' |
                                    datatype == 'numeric') &
                                   nacount == 0
                               ]
                      )

allcors.nona <- cor(train[, vars.num.nona], use='pairwise.complete.obs')
allcors.nona[is.na(allcors.nona)] <- 0

heatmap.2(allcors.nona, trace="none", col=greenred(10))

```


```{r allcors}

vars.num <- with(var.entropies,
                 variable[(datatype == 'integer' |
                               datatype == 'numeric')
                              ]
                 )

allcors <- cor(train[, vars.num], use='pairwise.complete.obs')
allcors[is.na(allcors)] <- 0

write.csv(allcors, file=paste(datapath,
                       'all_numeric_var_correlations_data_ecfixed.csv',
                       sep='')
          )
heatmap.2(allcors, trace="none", col=greenred(10))

```

#Kullback Leibler Divergence
KL Divergence can tell us how different two distributions are.

```{r kldfunc}

kld <- function(x, y, asymm = TRUE){
    if (!asymm) ( kld(x, y) + kld(y, x))/2
    else {
        vs <- union(unique(x), unique(y))
        cx = sapply(vs, function(v) sum(x==v))
        cy = sapply(vs, function(v) sum(y==v))
        px <- (cx + 1)/(sum(cx + 1))
        py <- (cy + 1)/sum(cy + 1)
        sum(px * log2(px/py))
    }
}

```

We will use this to compute the KL divergence for all the numeric
variables.

```{r kldnumvarstrain}

predictors <- names(train)[-1932]
predictors_num <- predictors[grepl(x=predictors, pattern="numeric")]
kld_num_train <- sapply(predictors_num, function(p){
                            t0 = train$target == 0
                            x = train[,p][t0]
                            y = train[,p][!t0]
                            kld(x[!is.na(x)], y[!is.na(y)])
                        })

```

We might find a high divergence between a variable, but the available
data in the column might be very small. We should store the number of
not NA values for each variable along with their KL divergences.
```{r kldnumnas}

kld_num_train_df <- data.frame(
    list(
        variable = predictors_num,
        kld = kld_num_train,
        notna1 = sapply(predictors_num, function(p) {
                            x <- train[,p][train$target == 1]
                            sum(!is.na(x))
                        }),
        notna0 = sapply(predictors_num, function(p) {
                            x <- train[,p][train$target == 0]
                            sum(!is.na(x))
                        }),
        distinct = sapply(predictors_num, function(p) {
                              length(unique(train[,p]))
                          })
        ),
    stringsAsFactors = FALSE
    )

kld_num_train_df <- kld_num_train_df[order(kld_num_train_df$distinct), ]

```
The variable _distinct_ in *kld_num_train_df* also counts NA. We will
create another variable in that data-frame that tells us if the
variable contains NAs, and will then add entropies, min, max, etc from
_var.entropies_.

```{r kldhasna}

kld_num_train_df$hasna <- sapply(kld_num_train_df$variable,
                                 function(p) any(is.na(train[, p])))

kld_num_train_df <- cbind(
    kld_num_train_df,
    var.entropies[kld_num_train_df$variable,
                  c('entropy', 'median', 'max', 'min')]
    )

```


```{r savekld}

save(kld_num_train_df, file="../data/kld_num_train_df.Rdata")

```
