
```{r setup, include=FALSE}

knitr::opts_chunk$set(cache=FALSE)
knitr::opts_chunk$set(echo=FALSE)
knitr::opts_chunk$set(include=TRUE)
knitr::opts_chunk$set(results="asis")
knitr::opts_chunk$set(fig.width=12)
knitr::opts_chunk$set(fig.height=14)
prjpath <- "~/work/learn/competitions/kaggle/springleaf/"
datapath <- paste(prjpath, "data/", sep="")
analpath <- paste(prjpath, "analysis/", sep="")
rcodepath <- paste(analpath, "Rcode/", sep="")
setwd(analpath)

```
```{r loadLibsAndSource, include=FALSE}

reqdpkgs <- c("lattice", "latticeExtra", "ggplot2", "reshape2",
              "plyr",  "lubridate", "Hmisc", "corrplot",
              "rpart", "rpart.plot", "RWeka",
              "caret", "gbm", "randomForest")
lapply(reqdpkgs, library, character.only=TRUE)

```
# Simple models

## A recipe to start modeling

Here we will develop a simple recipe to start modeling. Modeling
requires a lot of experimenting and hacking, and can lead to
incomprehensible code. We will try to develop good habits, documenting
as many of the ideas that we get in the process of modeling so that
they can be reproduced during the next session.

When any of our ideas produce decent results we can save our work as
both an independent Rmd, and as an R code script that can be use in
future modeling sessions.

We will consider several different model families here, which will be
further pursued in their own files. The goal is to get an idea on
which model families are suited for this dataset.

## Load data

We need to load data to start modeling. We can load directly from the
csvs, or we can use the saved Rdata which will be faster. For all of
our models we will use the labeled data,

```{r loaddata}

train <- load("../data/train_labeled.Rdata")
test <- load("../data/test_labeled.Rdata")

```

There are many predictors, and without helpful names. We will run our
models by passing the predictors as a vector of names. So we extract
the variables from the dataset,

```{r predictorlists}

V <- ncol(train)
predictors <- names(train)[-V] # the last variable is the outcome, target
predictor_types <- sapply(predictors, function(p) class(train[, p]))
predictors_chr <- predictors[predictor_types == 'character']
predictors_num <- predictors[predictor_types == 'integer' |
                                 predictor_types == 'numeric']
predictors_log <- predictors[predictor_types == 'logical']

```

Before we begin to investigate what models should be appropriate, we
need a baseline error to compare against. For the baseline, lets look
at how many 1s there are in the target.

```{r baseline}

baseline <- mean(train$target)
print( paste("Fraction of ones in the target", round(baseline,2)))

```
With only about 23% ones in the target, our default prediction of 0
would lead to an error on only 0.23. Any model worth the effort should
achieve an error larger than this value, hence the baseline of 0.23.

```{r baseline2}

print(paste("we have set a null model of target = 0 for any input"))
print(paste("this null model has a baseline error of ", baseline))

```

## Load scripts

We have several scripts that can be helpful for modeling. For example,
if we had started from raw data we would use the labeling scripts to
label the data for analysis. Instead we have already saved the labeled
data that we loaded above. We will need the functions defined in
_errorCodes.R_ which has been extracted from the Rmd _errorCodes.Rmd_,

```{r loadscripts}

source("../Rcode/errorCodes.R")
source("../Rcode/correlations.R")

```

Any other scripts that we may write and want to include should also go
above.

## Error-codes

Not all the values in the numerical columns are real values. Some are
error-codes, that we written a function to identify. We can pass
arguments to that function to obtain the error-codes for different
criteria. We have already figured out a set of arguments, that we use
for our current analysis,

```{r ovl}

ovl_10_00001<- lapply(predictors,
                 function(var) {
                     print("outliers for ")
                     ol <- dpoutlyers(train[, var], offset=10, rarity=1e-5)
                     print(var)
                     print(ol)
                 }
                 )

names(ovl_10_00001) <- predictors

```

We will create datasets without the error-codes, which can be used for
a cleaner comparison of variables, and also for modeling. Once we have
better grasp on models that work, we will decide what to do with the
information in the error-codes.

```{r removeec}

train.noec <- train
test.noec <- test
for (p in names(ovl_10_00001)) {
         x <- train[,p]
         x[ !(x %in% ovl_10_00001[[p]]) ] <- NA
         train.noec[,p] <- x
         y <- test[,p]
         y[ !(y %in% ovl_10_00001[[p]]) ] <- NA
         test.noec[,p] <- y
     }

```

## Data preparation for modeling

We have the datasets, but they are not ready for modeling. As a
general procedure, we want to keep the original datasets untouched. So
we copy them into model sets, and make lists of suitable predictors
according to their datatypes and other properties that we find
suitable.

**Some of the character variables are problematic. These are the job and
location descriptors of the rows. May be these contain a lot of
relevant information, but will require special treatment before they
can be deployed for prediction.**

### Numerical to categorical

Some of the numerical variables are best considered as categorical,
and others as genuine continuous numerical. Which ones are which? We
have studied this question to quite some detail in _numCat.Rmd_, and
will use the results from there. For the following code, we assume
that the code in _numCat.Rmd_ has been run and the variables available
in the R environment. Some of the numerical variables seem to have a
large mass at value 0. This makes the variable look like a categorical
plot in the histograms, however a closer examination reveals the
log-normal in the remaining mass of their distribution. To this end we
will use both the fingerprints with and without value 0 for the DP
fields.

```{r cutsummaryplotWithZero}

n <- 5
csm <- cutsummaries(n, var.fpnormed)
csm <- csm
is.dpfield <- grepl(pattern="dp", x=row.names(csm))
#for(i in 1:n) {
 #   csm[, i] <- c(log10(1 + csm[ is.dpfield, i]), csm[!is.dpfield,i])
#}
csm.df <- data.frame(csm)
csm.df$fpfield <- row.names(csm)
csm.df <- subset(csm.df, !(fpfield %in% c("nna", "nec", "neg")))
#csm.df$fpfield <- as.factor(csm.df$fpfield)
csm.df$fpfield <- factor(csm.df$fpfield,
                         levels=c(paste("dp", 0:11, sep="."),
                             with(csm.df,
                                  fpfield[ !grepl(pattern="dp", fpfield)])
                                  )
                         )
csm.df$fpclass <- c("ST", "DP")[1 + grepl(pattern="dp",
                                          x=as.character(csm.df$fpfield))]
csm.df$fpclass <- as.factor(csm.df$fpclass)
csm.df.ml <- melt(csm.df, id.vars=c("fpfield", "fpclass"))
ap0 <- xyplot(value ~ fpfield | fpclass,
              groups=variable,
              data=csm.df.ml,
              auto.key=TRUE,
              type="b",
              scales=list(x=list(rot=90), pch=25, relation="free"),
              main="fingerprints for clusters, including value 0 for class DP"
              )

print(ap0)
pdf("figures/cutsummaryFingerPrintsK5_wtZero.pdf")
print(ap0)
dev.off()

```
When the value zero is excluded,

```{r cutsummaryplotN0}

n <- 10
csm <- cutsummaries(n, var.fpnz)
is.dpfield <- grepl(pattern="dp", x=row.names(csm))
#for(i in 1:n) {
 #   csm[, i] <- c(log10(1 + csm[ is.dpfield, i]), csm[!is.dpfield,i])
#}
csm.df <- data.frame(csm)
csm.df$fpfield <- row.names(csm)
csm.df <- subset(csm.df, !(fpfield %in% c("nna", "nec", "neg")))
#csm.df$fpfield <- as.factor(csm.df$fpfield)
csm.df$fpfield <- factor(csm.df$fpfield,
                         levels=c(paste("dp", 1:11, sep="."),
                             with(csm.df,
                                  fpfield[ !grepl(pattern="dp", fpfield)])
                                  )
                         )
csm.df$fpclass <- c("ST", "DP")[1 + grepl(pattern="dp",
                                          x=as.character(csm.df$fpfield))]
csm.df$fpclass <- as.factor(csm.df$fpclass)
csm.df.ml <- melt(csm.df, id.vars=c("fpfield", "fpclass"))
apn0 <- xyplot(value ~ fpfield | fpclass,
               groups=variable,
               data=csm.df.ml,
               auto.key=TRUE,
               type="b",
               scales=list(x=list(rot=90), pch=25, relation="free"),
               main=" fingerprints for clusters, without the value 0 for class DP"
               )
print(apn0)

pdf("figures/cutsummaryFingerPrintsK5_woZero.pdf")
print(apn0)
dev.off()

```

We considered only the normalized DP fields. We can cluster with the
unnormalized fingerprints, which contain the actual number of samples
in the DP bins,

```{r cutsummaryplotUnnormed}

n <- 7
fpfield <- colnames(var.fingerprints)
var.fingerprints.noz <- var.fingerprints[, fpfield != "dp.0"]
csm <- cutsummaries(n, var.fingerprints.noz)
is.dpfield <- grepl(pattern="dp", x=row.names(csm))
for(i in 1:n) {
    csm[, i] <- c(log10(1 + csm[ is.dpfield, i]), csm[!is.dpfield,i])
}
csm.df <- data.frame(csm)
csm.df$fpfield <- row.names(csm)
csm.df <- subset(csm.df, !(fpfield %in% c("nna", "nec", "neg")))
#csm.df$fpfield <- as.factor(csm.df$fpfield)
csm.df$fpfield <- factor(csm.df$fpfield,
                         levels=c(paste("dp", 1:11, sep="."),
                             with(csm.df,
                                  fpfield[ !grepl(pattern="dp", fpfield)])
                                  )
                         )
csm.df$fpclass <- c("ST", "DP")[1 + grepl(pattern="dp",
                                          x=as.character(csm.df$fpfield))]
csm.df$fpclass <- as.factor(csm.df$fpclass)
csm.df.ml <- melt(csm.df, id.vars=c("fpfield", "fpclass"))
apn0 <- xyplot(value ~ fpfield | fpclass,
               groups=variable,
               data=csm.df.ml,
               auto.key=TRUE,
               type="b",
               scales=list(x=list(rot=90), pch=25, relation="free"),
               main=" fingerprints for clusters, without the value 0 for class DP"
               )
print(apn0)

pdf("figures/cutsummaryFingerPrintsK5_unnoremed.pdf")
print(apn0)
dev.off()

```

We have used several different criteria to obtain our clusters.
We will use the cluster when we excluded zero values to determine the
variable type. From the cluster summaries in the plot we see clearly that cluster 1 (X1)
should be continuous numerical, and clusters 3 and 4
categorical. Clusters 2 and 5 are tricky.

Cluster 5 has lots of values
between 100 and 1000, but only less than 32  (2 to the power 5)
distinct values on the average. Their mean entropy is low
at about 2 bits, and only increases slightly when error-codes and
zeros are excluded.

Clusters 2 may have values larger than $10^4$, and
upto 500 distinct values ($2^9$), and entropy more than 4 bits when
all values are included, 6 bits when error-codes are dropped, and
jumps to 7.5 bits when zero is also dropped.

To automate our observations, we can just cluster the cluster
summaries! We will use all the summaries, or only the DP summaries.

```{r cluclusum}

n <- 5
csm <- cutsummaries(n, var.fpnz)
plot(hclust(dist(t(csm[!is.dpfield,]), method="euclidean"),
            method="ward"),
     main="clusters for the cluster DP summaries in var.fpnz"
     )

plot(hclust(dist(t(csm), method="euclidean"),
            method="ward"),
          main="clusters for the cluster summaries, both DP and ST in var.fpnz"
     )

```

In either case we find that clusters 1 and 2 should go together as
continuous numeric, and 3,4,5 group together and we will declare them
as categorical.

Ot alternatively we can group 1,2, and 5 as continuous.

```{r setdatatypes}

d <- dist(scale(var.fpnz), method='euclidean')
clu <- hclust(d, method="ward")
clucut <- cutree(clu, k=5)
preds.model.num.cons <- rownames(var.fpnz)[clucut == 1 | clucut == 2 | clucut == 5]
preds.model.num.cats <- rownames(var.fpnz)[clucut == 3 | clucut == 4]

```

We will replace NAs by unknowns, for all categorical variables.

```{r modeldataandpreds}

train.model <- train
test.model <- test

preds.model.dates <- predictors_chr[ grepl(x=predictors_chr, pattern='date')]
preds.model.cats <- c('hrq', 'cbns', 'abcdefu',
                      'shrug_first', 'shrug_second', 'shrug_third',
                      'oru_first', 'oru_second', 'oru_third',
                      'eye', 'status', 'medium', 'idused')
preds.model.jobs <- c('job_first', 'job_second')
preds.model.locs <- c('state_first', 'state_second', 'city')
preds.model.logs <- predictors_log

## set dates from strings

for(p in preds.model.dates) {
    train.model[,p] <- ymd(train.model[,p])
}

for(p in preds.model.num.cats) {
    x <- train.model[,p]
    x[is.na(x)] <- 'unknown'
    train.model[,p] <- as.factor(x)
}

for (p in preds.model.cats) {
    x <- train.model[,p]
    x[is.na(x)] <- 'unknown'
    train.model[,p] <- as.factor(x)
}

## year of date as a factor

for (p in preds.model.dates) {
    py <- paste(p, 'year', sep='.')
    x <- as.character(year(train.model[,p]))
    x[is.na(x)] <- 'unknown'
    train.model[, py] <- as.numeric(x)
}

## remove error-codes for numerical continuous vars

for (p in preds.model.num.cons) {
    xs <- train.model[, p]
    xs[ xs%in% ovl_10_00001[[p]]] <- NA
    xs[xs < 0] <- NA
    train.model[,p] <- log10(1 + xs)
}

preds.model.dates.year <- paste(preds.model.dates,
                                'year', sep='.')

```

Now that we added new columns as years of the dates, we need to add
these to the dateframes for variable properties.

```{r yeartovarents}

var.entropies.year <- predictor.entropies(preds.model.dates.year,
                                           data=train.model)

var.entropies <- rbind(var.entropies, var.entropies.year[, names(var.entropies)])

kld.allvars <- kld.vars(predictors, data=train)
kld.years <- kld.vars(preds.model.dates.year, data=train.model)
kld.allvars <- rbind(kld.allvars, kld.years)
kld.allvars <- with(kld.allvars, kld.allvars[ order(kld/entropy, decreasing=TRUE), ])

```


## Value in Zeros, and error-codes

We have noticed a large number of zeros in the data, and have
evaluated variable summaries without them. However, for the KLD we did
not remove these. In addition to removing them we should also look at
the statistics if we keep them, and remove all other values. What we
mean will be clearer when we discuss specific cases.

Consider a numerical variable which has many 0s. What is the
information in its value of 0? We can measure that if we compute the
distribution of the target over the subsets where the variable takes
the value of 0 or not.

```{r zeroornotnumeric}

N <- nrow(train)
q <- 0.2
pzs <- Filter(function(p) {
                  nz  <- sum(train[,p] == 0, na.rm=TRUE)/N
                  no <- sum(train[,p] != 0, na.rm=TRUE)/N
                  nz >= q & nz < 1 - q
              }, predictors_num
              )

prob.1.pzs <- do.call(rbind, lapply(pzs, function(p) {
                                        df <- train[, c('target', p)]
                                        names(df) <- c('target', 'varval')
                                        df$varval <- df$varval != 0
                                        df$varval[ is.na(df$varval) ] <- TRUE
                                        tdf <- table(df)
                                        print(tdf)
                                        t(t(tdf)/colSums(tdf))[2,]
                                    })
                      )
colnames(prob.1.pzs) <- c("V0", "V1")
prob.1.pzs <- as.data.frame(prob.1.pzs)
prob.1.pzs$variable <- paste(pzs, "nz", sep=".")
prob.1.pzs$odds.V1 <- with(prob.1.pzs, V1/(1-V1))
prob.1.pzs$odds.V0 <- with(prob.1.pzs, V0/(1-V0))
prob.1.pzs <- prob.1.pzs[ order(prob.1.pzs$odds.V0, prob.1.pzs$odds.V1, decreasing=TRUE),]


```

We can make a dataset with columns that tell us if the variable is 0
or not

```{r dataset0ornot}

train.znz <- train[, pzs] != 0
train.znz[ is.na(train.znz) ] <- TRUE
train.znz <- as.data.frame(train.znz)
names(train.znz) <- sapply(names(train.znz), function(p) paste(p, 'nz', sep="."))

train.znz$target <- train$target

fit.rpart.znz <- rpart(target ~ ., data=train.znz,
                       control=rpart.control(depth=100, cp=0.001, minsplit=10))


prd <- predict(fit.rpart.znz, newdata=train, method="response")
print("our training auc for the roc", roc.auc(prd, train$target)

```

### Error codes
We can also create data-sets that take into account the error-codes

```{r includeec}

q <- 0.1
pecs <- Filter(function(p) {
                   xs <- train[, p]
                   ecs <- ovl_10_00001[[p]]
                   sum(xs %in% ecs, na.rm=TRUE) >= q
               }, predictors_num
               )
train.ec <- data.frame(lapply(pecs,
                              function(p) {
                                  xs <- train[,p]
                                  ecs <- ovl_10_00001[[p]]
                                  isec <- xs %in% ecs
                                  xs[!isec] <- 0
                                  as.factor(xs)
                              })
                       )
names(train.ec ) <- paste(pecs, "ec", sep=".")

```

```{r treeWithEC}

train.ec$target <- train$target
fit.rpart.ec <- rpart(target ~ ., data=train.ec,
                      control=rpart.control(cp=0.001, minsplit=10, depth=100)
                      )

```






## Tree Models
If try all, there will be too many variables. So we drop some

```{r dropvars}

for (p in preds.model.dates) {
    train.model[,p] <- as.factor(train.model[,p])
}

preds.model <- setdiff(
    unique(c(preds.model.num.cons, preds.model.num.cats,
             preds.model.logs, preds.model.cats,
             preds.model.dates.year)),
    c("X", "ID")
    )

keep.preds.model <- sapply(preds.model,
                           function(p){
                              k <- ( var.entropies[p, 'entropy'] > 0.1 &
                                        var.entropies[p, 'nacount'] < 0.5 * nrow(train.model) )
                              k
                           })


fit.rpart <- rpart(target ~ .,
                   data = train.model[, c(preds.model[keep.preds.model], "target")]
                   )

```

The first model was the same as the baseline model. We will next
consider top variables by their relative kld/entropy.




### Rpart 1: Use variables with high relative kld

```{r highrelativekld}

preds.model.1 <- with(kld.allvars, variable[ entropy > 0.5 &
                                              notna1 + notna0 > 1.4e5][1:200])


preds.model.2 <- Filter(function(p) kld.allvars[p, 'distinct'] > 1000 & kld.a,
                        preds.model.num
                        )

preds.model <- union(preds.model.1, preds.model.2)

fit.rpart.1 <- rpart(target ~ .,
                     data = train.model[, c(preds.model, "target")],
                     )

```

### Rpart 2: Use variable with high kld

```{r highkld}

kld.allvars <- kld.allvars[order(kld.allvars$kld, decreasing=TRUE), ]
preds.model <- kld.allvars$variable[1:30]

fit.rpart.2 <- rpart(target ~ .,
                     data=train.model[, c(preds.model, "target")],
                     control=rpart.control(
                         minsplit=5,
                         cp=0.01,
                         maxdepth=10
                         )
                     )

```

### Rpart 3: Choose the predictors by their number of distinct values

```{r choosebynumberofdistinct}

naentfilter <- function(data, notnanum = 1e5, e = 0.1){
    with(data, (notna1 + notna0 > notnanum) & entropy > e)
}
kldfilter <- function(data, k=0.01){
    with(data, kld > k)
}


preds.model.n <- with(kld.allvars,
                      variable[naentfilter(kld.allvars) &
                                   kldfilter(kld.allvars)]
                      )
preds.model.2 <- with(kld.allvars[preds.model.n, ],
                      variable[distinct == 2])
preds.model.3 <- with(kld.allvars[preds.model.n, ],
                      variable[distinct == 3])
preds.model.4 <- with(kld.allvars[preds.model.n, ],
                      variable[distinct == 4])
preds.model.5 <- with(kld.allvars[preds.model.n, ],
                      variable[distinct == 5])

preds.model <- c(preds.model.1, preds.model.2,
                 preds.model.3, preds.model.4,
                 preds.model.5)

fit.rpart <- rpart(target ~ .,
                   data = train.model[, c('target', preds.model)]
                   )

prp(fit.rpart, varlen=0)

```

## Logistic regression

Lets first consider the numeric  variables with two distinct values. Some of
these are useless because the two values for these variables consist
of an actual value and an NA. I am not sure what we can do with these
values, but let us try making a logistic regression model anyway,

```{r logisiticWith2ValVariables}

vals2numericVars <- with(num_vals_num_vars, label[num_vals == 2])
m.numeric2 <- glm(target ~ .,
                  data = train[, c(vals2numericVars, 'target')],
                  family='binomial'
                  )
tp.numeric2 <- predict(m.numeric2, newdata=train, type='response')
err.numeric2 <- sum( (tp.numeric2 > 0.5) != train$target, na.rm=TRUE)/nrow(train)

print("the logistic model with only numeric variables that take 2 values")
print(paste("training error :  ", round(err.numeric2,2)))
print(paste("compared to baseline: ", round(err.numeric2/baseline, 2)))
```
**No better than the null**


```{r logisiticWith3ValVariables}

vals3numericVars <- with(num_vals_num_vars, label[num_vals == 3])
m.numeric3 <- glm(target ~ .,
                  data = train.ecfixed[, c(vals3numericVars, 'target')],
                  family='binomial'
                  )
tp.numeric3 <- predict(m.numeric3, newdata=train.ecfixed, type='response')
err.numeric3 <- sum( (tp.numeric3 > 0.5) != train$target, na.rm=TRUE)/nrow(train)

print("the logistic model with only numeric variables that take 3 values")
print(paste("training error :  ", round(err.numeric3,2)))
print(paste("compared to baseline: ", round(err.numeric3/baseline, 2)))

```

Lets automate this process,

```{r logisticWithNvalsVars}

for(n in 2:max(var.entropies$distinct_vals)) {
    predictors <- Filter(
        function(p) {
            var.entropies[p, 'entropy'] > 0.1 &
                var.entropies[p, 'nacount'] < 1000
        },
        with(var.entropies, variable[distinct_vals == n &
                                         (datatype == 'numeric' |
                                              datatype == 'integer')])
        )
    print(paste("to try predictors with ", n , "distinct values"))
    print(paste("total number of such predictors ", length(predictors)))
    if(length(predictors) > 0) {
        m <- glm(target ~ .,
                 data = train[, c(predictors, 'target')],
                 family='binomial'
                 )
        tp <- predict(m, newdata=train, type='response')
        err <- sum( (tp > 0.5) != train$target, na.rm=TRUE)/nrow(train)
        print(paste(" logistic model with only num vars that take ", n, " values"))
        print(paste("total number of variables considered", length(predictors)))
        print(paste("training error :  ", round(err,2)))
        print(paste("compared to baseline: ", round(err/baseline, 2)))
        print("|||||||||||||||||||||||||||||||||||||||")
    }
}


```{r cors1}

cors1 <- cor(train[, num_vals_num_vars$label[127:136]], use="pairwise.complete.obs")

```
