```{r setup, include=FALSE}

knitr::opts_chunk$set(cache=FALSE)
knitr::opts_chunk$set(echo=FALSE)
knitr::opts_chunk$set(include=TRUE)
knitr::opts_chunk$set(results="asis")
knitr::opts_chunk$set(fig.width=12)
knitr::opts_chunk$set(fig.height=14)
prjpath <- "~/work/learn/competitions/kaggle/springleaf/"
datapath <- paste(prjpath, "data/", sep="")
analpath <- paste(prjpath, "analysis/", sep="")
rcodepath <- paste(analpath, "Rcode/", sep="")
setwd(analpath)


```
```{r loadLibsAndSource, include=FALSE}

reqdpkgs <- c("lattice", "latticeExtra", "ggplot2", "reshape2",
              "plyr",  "lubridate", "Hmisc", "corrplot",
              "rpart", "rpart.plot", "RWeka",
              "caret", "gbm", "randomForest")
lapply(reqdpkgs, library, character.only=TRUE)

```
# Simple models

## A recipe to start modeling

Here we will develop a simple recipe to start modeling. Modeling
requires a lot of experimenting and hacking, and can lead to
incomprehensible code. We will try to develop good habits, documenting
as many of the ideas that we get in the process of modeling so that
they can be reproduced during the next session.

When any of our ideas produce decent results we can save our work as
both an independent Rmd, and as an R code script that can be use in
future modeling sessions.

We will consider several different model families here, which will be
further pursued in their own files. The goal is to get an idea on
which model families are suited for this dataset.

## Load data

We need to load data to start modeling. We can load directly from the
csvs, or we can use the saved Rdata which will be faster. For all of
our models we will use the labeled data,

```{r loaddata}

train <- load("../data/train_labeled.Rdata")
test <- load("../data/test_labeled.Rdata")

```

There are many predictors, and without helpful names. We will run our
models by passing the predictors as a vector of names. So we extract
the variables from the dataset,

```{r predictorlists}

V <- ncol(train)
predictors <- names(train)[-V] # the last variable is the outcome, target
predictor_types <- sapply(predictors, function(p) class(train[, p]))
predictors_chr <- predictors[predictor_types == 'character']
predictors_num <- predictors[predictor_types == 'integer' |
                                 predictor_types == 'numeric']
predictors_log <- predictors[predictor_types == 'logical']

```

Before we begin to investigate what models should be appropriate, we
need a baseline error to compare against. For the baseline, lets look
at how many 1s there are in the target.

```{r baseline}

baseline <- mean(train$target)
print( paste("Fraction of ones in the target", round(baseline,2)))

```
With only about 23% ones in the target, our default prediction of 0
would lead to an error on only 0.23. Any model worth the effort should
achieve an error larger than this value, hence the baseline of 0.23.

```{r baseline2}

print(paste("we have set a null model of target = 0 for any input"))
print(paste("this null model has a baseline error of ", baseline))

```

## Load scripts

We have several scripts that can be helpful for modeling. For example,
if we had started from raw data we would use the labeling scripts to
label the data for analysis. Instead we have already saved the labeled
data that we loaded above. We will need the functions defined in
_errorCodes.R_ which has been extracted from the Rmd _errorCodes.Rmd_,

```{r loadscripts}

source("../Rcode/errorCodes.R")
source("../Rcode/correlations.R")

```

Any other scripts that we may write and want to include should also go
above.

## Error-codes

Not all the values in the numerical columns are real values. Some are
error-codes, that we written a function to identify. We can pass
arguments to that function to obtain the error-codes for different
criteria. We have already figured out a set of arguments, that we use
for our current analysis,

```{r ovl}

ovl_10_00001<- lapply(predictors,
                 function(var) {
                     print("outliers for ")
                     ol <- dpoutlyers(train[, var], offset=10, rarity=1e-5)
                     print(var)
                     print(ol)
                 }
                 )

names(ovl_10_00001) <- predictors

```

We will create datasets without the error-codes, which can be used for
a cleaner comparison of variables, and also for modeling. Once we have
better grasp on models that work, we will decide what to do with the
information in the error-codes.

```{r removeec}

train.noec <- train
test.noec <- test
for (p in names(ovl_10_00001)) {
         x <- train[,p]
         x[ !(x %in% ovl_10_00001[[p]]) ] <- NA
         train.noec[,p] <- x
         y <- test[,p]
         y[ !(y %in% ovl_10_00001[[p]]) ] <- NA
         test.noec[,p] <- y
     }

```

## Data preparation for modeling

We have the datasets, but they are not ready for modeling. As a
general procedure, we want to keep the original datasets untouched. So
we copy them into model sets, and make lists of suitable predictors
according to their datatypes and other properties that we find
suitable.

**Some of the character variables are problematic. These are the job and
location descriptors of the rows. May be these contain a lot of
relevant information, but will require special treatment before they
can be deployed for prediction.**

We will replace NAs by unknowns, for all categorical variables.

```{r modeldataandpreds}

train.model <- train
test.model <- test

numcutoff <- 10

preds.model.num <- with(var.entropies,
                        variable[ (datatype=='numeric' |
                                       datatype=='integer') &
                                     distinct_vals > numcutoff]
                        )

preds.model.numcat <- with(var.entropies,
                           variable[ (datatype=='numeric' |
                                          datatype=='integer') &
                                        distinct_vals <= numcutoff]
                           )

preds.model.dates <- predictors_chr[ grepl(x=predictors_chr, pattern='date')]

preds.model.cats <- c('hrq', 'cbns', 'abcdefu',
                      'shrug_first', 'shrug_second', 'shrug_third',
                      'oru_first', 'oru_second', 'oru_third',
                      'eye', 'status', 'medium', 'idused')
preds.model.jobs <- c('job_first', 'job_second')
preds.model.locs <- c('state_first', 'state_second', 'city')

preds.model.logs <- predictors_log

## set dates from strings

for(p in preds.model.dates) {
    train.model[,p] <- ymd(train.model[,p])
}

for(p in preds.model.numcat) {
    x <- train.model[,p]
    x[is.na(x)] <- 'unknown'
    train.model[,p] <- as.factor(x)
}

for (p in preds.model.cats) {
    x <- train.model[,p]
    x[is.na(x)] <- 'unknown'
    train.model[,p] <- as.factor(x)
}

## year of date as a factor

for (p in preds.model.dates) {
    py <- paste(p, 'year', sep='.')
    x <- as.character(year(train.model[,p]))
    x[is.na(x)] <- 'unknown'
    train.model[, py] <- as.factor(x)
}

preds.model.dates.year <- paste(preds.model.dates,
                                'year', sep='.')

```

Now that we added new columns as years of the dates, we need to add
these to the dateframes for variable properties.

```{r yeartovarents}

var.entropies.year <- predictor.entropies(preds.model.dates.year,
                                           data=train.model)

var.entropies <- rbind(var.entropies, var.entropies.year[, names(var.entropies)])

kld.allvars <- kld.vars(predictors, data=train)
kld.years <- kld.vars(preds.model.dates.year, data=train.model)
kld.allvars <- rbind(kld.allvars, kld.years)
kld.allvars <- with(kld.allvars, kld.allvars[ order(kld/entropy, decreasing=TRUE), ])

```

If try all, there will be too many variables. So we drop some

```{r dropvars}

preds.model <- setdiff(
    unique(c(preds.model.num, preds.model.numcat,
             preds.model.logs, preds.model.cats,
             preds.model.dates.year)),
    c("X", "ID")
    )

keep.preds.model <- sapply(preds.model,
                           function(p){
                              k <- ( var.entropies[p, 'entropy'] > 0.1 &
                                        var.entropies[p, 'nacount'] < 0.5 * nrow(train.model) )
                              k
                           })


train.model$target <- as.factor(train.model$target)
fit.rpart <- rpart(target ~ .,
                   data = train.model[, c(preds.model[keep.preds.model], "target")],
                   method='class'
                   )

```

The first model was the same as the baseline model. We will next
consider top variables by their relative kld/entropy.


### Numerical variables on a log scale

We have seen in the histograms that several variables are better
characterized on a log-scale. Lets build a scheme to plot several of
these together and browse through them,

```{r logscalehistos}

numcutoff <- 1
pnums <- with(var.entropies,
              variable[entropy > 0 & distinct_vals > numcutoff &
                           (datatype == 'integer' | datatype == 'numeric')])



n <- 1
dn <- 16
tdf <- noec.df(train[ , pnums[n:(n+dn - 1)]])
ns <- names(tdf)
names(tdf) <- paste(ns,
                    "dis",
                    var.entropies[ns, 'distinct_vals'],
                    sep="."
                    )
histogram(~ log10(1 + value) | variable,
          data = melt(tdf),
          type = "density",
          par.settings = list(
              par.main.text=list(cex=1),
              superpose.symbol=list(cex=0.5),
              fontsize=list(text = 8, points = 4)
              ),
          scales = list(par.sub.text = 1),
          main = "log scale histogram for numeric variables"
          )
names(tdf) <- ns
print(do.call(rbind, lapply(tdf, function(xs) {
                            c(min = min(xs, na.rm=TRUE),
                              median = median(xs, na.rm=TRUE),
                              max = max(xs, na.rm=TRUE),
                              distinct = length(unique(xs))
                              )
                        })
                 )
      )

```
We can use the function _plotHistsForGivenDistinctVals_, defined in
_correlations.R_ to plot histograms. Browsing through these plots will
give us an idea what the variables represent. For  variables with a
few distinct values we find very descrete bars,

```{r plotfewdistinct}

plotHistsForGivenDistinctVals(from.index=100, breaks=NULL)

```

At the other end are the _continuous_ variables, which look like
log-normals in the plots.

```{r plotmanydistinct}

plotHistsForGivenDistinctVals(from.index=1700, breaks=NULL)

```
Notive the variables with a single large bar at 0 and a much smaller
mass at larger values. When magnified by plotting the histogram with
the 0 removed, we see again that these plots are also log-normal,

```{r manydistinctzeroremoved}

plotHistsForGivenDistinctVals(from.index=1700, breaks=NULL, zeroRemoved=TRUE)

```

We see that even the value 0 may represent some kind of an error-code
in these data. Or may be these data represent a monthly/yearly payment
made. In that case 0, with the highest mass, would be records that
have not made any payment.

**We should deal with the zero value of some of the variables,
  especially the ones with many distinct values, as the value zero may
  represent absence of something, rather than simply a numeric value
  of zero. Furthermore, the value zero changes the shape of a nice
  log-normalisch curve that rest of the values follow.**


Between the two extremes of a few (< 10) and many valued variables ( ~
$10^4$), are the variables with a moderate number of values ( ~ 50).

```{r plotmoderatevaluedvars}

plotHistsForGivenDistinctVals(from.index=1000, breaks=NULL)

```
Some have a large mass at 0, which we can remove

```{r plotmoderatevaluedvarszeroremoved}

plotHistsForGivenDistinctVals(from.index=1000, breaks=NULL, zeroRemoved=TRUE)

```

Many of these show an exponential distribution, making it harder to
determine if these should be categorical or numeric.

## Cluster the variables

Now, after plotting a lot of histograms, we are ready to cluster
them. The first clustering we will do is by looking at statistical
summaries of the variables.

```{r clustervarsummaries}
preds.clu <- predictors_num
var.summaries <- do.call(rbind, lapply(preds.clu, function(p){
                             xs <- train[, p]
                             nna <- sum(is.na(xs))/nrow(train)
                             xs <- xs[!is.na(xs)]
                             xs.r <- removedValues(xs, ovl_10_00001[[p]])
                             xs.z <- removedValues(xs.r, 0)
                             c(dis=log2(length(unique(xs))),
                               nna=nna,
                               min=log2(min(xs)),
                               med=log2(median(xs)),
                               max=log2(max(xs)),
                               ent=entropy(xs),
                               dis.noec=log2(length(unique(xs.r))),
                               min.noec=log2(1 + min(xs.r)),
                               med.noec=log2(1 + median(xs.r)),
                               max.noec=log2(1 + max(xs.r)),
                               ent.noec=entropy(xs.r),
                               dis.noz=log2(length(unique(xs.z))),
                               min.noz=log2(1 + min(xs.z)),
                               med.noz=log2(1 + median(xs.z)),
                               max.noz=log2(1 + max(xs.z)),
                               ent.noz=entropy(xs.z)
                               )
                         }))
var.summaries[is.infinite(var.summaries)] <- NA
row.names(var.summaries) <- preds.clu
dist.preds <- dist(scale(var.summaries), method='euclidean')
clu.preds <- hclust(dist.preds, method='ward')
plot(clu.preds)

```

To read the clustering, we can cut the tree and look at the means of
the summaries

```{r cutsummaries}
cutsummaries <- function(k, data=var.summaries) {
    d <- dist(scale(data), method='euclidean')
    clu <- hclust(d, method='ward')
    plot(clu, labels=FALSE)
    clucut <- cutree(clu, k=k)
    do.call(cbind,
            lapply(unique(clucut),
                   function(c) colMeans(data[ clucut == c,], na.rm=TRUE)
                   )
            )
}

```
Another way to cluster the variables is to bin the data for each
predictor, into the same bins. We will store the total number of samples
at a given bin, and get the mean values for the bins when we analyze them.

```{r varfingerprints}

pfs <- predictors_num
var.fingerprints <- do.call(rbind,
                            lapply(pfs,
                                   function(p) {
                                       bins <- rep(0, 12)
                                       names(bins) = 0:11
                                       xs <- train[, p]
                                       xs <- xs[ xs >= 0]
                                       xs <- xs[!is.na(xs)]
                                       lxs <- table(floor(log10(1+xs)))
                                       bins[names(lxs)] <- as.numeric(lxs)
                                       bins
                                   })
                            )
colnames(var.fingerprints) <- paste("dp", 0:11, sep=".")
neg <- sapply(pfs, function(p) {
                  sum(train[,p] < 0, na.rm=TRUE)
              })
var.fingerprints <- cbind(var.fingerprints, neg)
nna <- sapply(pfs, function(p) sum(is.na(train[,p])))
var.fingerprints <- cbind(var.fingerprints, nna)

nec <- sapply(pfs, function(p) {
                  xs <- train[,p]
                  xs <- xs[!is.na(xs)]
                  sum(xs %in% ovl_10_00001[[p]])
              })
var.fingerprints <- cbind(var.fingerprints, nec)
var.fingerprints <- cbind(var.fingerprints, var.summaries)
row.names(var.fingerprints) <- pfs

```

To use the finger-prints for clustering, we will normalize the rows by
their means. Because, many continuous numeric variables assume a value
zero that stands apart from rest of the log-normally distributed data,
we will not consider the zeroes for the means. We dont have to compute
the finger-prints for this. The linear-scale 0 remains a zero in our
logscale ( log10(1 + xs) ).

```r{ fpnz}

fpfields <- colnames(var.fingerprints)
is.dpfield <- grepl(pattern="dp", x=fpfields)
var.fpnz <- var.fingerprints[, fpfields[is.dpfield]]
rs <- rowSums(var.fpnz)
var.fpnz <- var.fpnz/rs
var.fpnz[is.nan(var.fpnz)] <- 0
var.fpnz <- cbind(var.fpnz,
                  var.fingerprints[, fpfields[!is.dpfield]]
                  )


dfp <- dist(scale(var.fpnz), method='euclidean')
cfp <- hclust(dfp, method='ward')
plot(cfp, labels=FALSE)

```

We can print mean values of the bins after clustering the bin
finger-prints.

```{r fpplot}

n <- 5
cutfp <- as.data.frame(cutsummaries(n, var.fpnz))
names(cutfp) <- paste('cluster', 1:n, sep=".")
cutfp.dp <- cutfp[grepl(pattern="dp", x=row.names(cutfp)), ]
cutfp.dp$size <- sapply(strsplit(row.names(cutfp.dp), split=".", fixed=TRUE),
                        function(xs) as.numeric(xs[2]))

xyplot(value ~ size, groups=variable,
       data=melt(cutfp.dp, id.vars='size'),
       type="b",
       auto.key=TRUE
       )

```





## Rpart 1: Use variables with high relative kld

```{r highrelativekld}

preds.model.1 <- with(kld.allvars, variable[ entropy > 0.5 &
                                              notna1 + notna0 > 1.4e5][1:200])


preds.model.2 <- Filter(function(p) {
                            with(kld.allvars[p,], distinct > 1e5 &
                                     notna1 + notna0 > 1.4e5)
                        },
                        preds.model.num
                        )

preds.model <- union(preds.model.1, preds.model.2)

fit.rpart.1 <- rpart(target ~ .,
                     data = train.model[, c(preds.model, "target")],
                     )

```

### Rpart 2: Use variable with high kld

```{r highkld}

kld.allvars <- kld.allvars[order(kld.allvars$kld, decreasing=TRUE), ]
preds.model <- kld.allvars$variable[1:30]

fit.rpart.2 <- rpart(target ~ .,
                     data=train.model[, c(preds.model, "target")],
                     control=rpart.control(
                         minsplit=5,
                         cp=0.01,
                         maxdepth=10
                         )
                     )

```

### Rpart 3: Choose the predictors by their number of distinct values

```{r choosebynumberofdistinct}

naentfilter <- function(data, notnanum = 1e5, e = 0.1){
    with(data, (notna1 + notna0 > notnanum) & entropy > e)
}
kldfilter <- function(data, k=0.01){
    with(data, kld > k)
}


preds.model.n <- with(kld.allvars,
                      variable[naentfilter(kld.allvars) &
                                   kldfilter(kld.allvars)]
                      )
preds.model.2 <- with(kld.allvars[preds.model.n, ],
                      variable[distinct == 2])
preds.model.3 <- with(kld.allvars[preds.model.n, ],
                      variable[distinct == 3])
preds.model.4 <- with(kld.allvars[preds.model.n, ],
                      variable[distinct == 4])
preds.model.5 <- with(kld.allvars[preds.model.n, ],
                      variable[distinct == 5])

preds.model <- c(preds.model.1, preds.model.2,
                 preds.model.3, preds.model.4,
                 preds.model.5)

fit.rpart <- rpart(target ~ .,
                   data = train.model[, c('target', preds.model)]
                   )

prp(fit.rpart, varlen=0)

```
## Logistic regression

Lets first consider the numeric  variables with two distinct values. Some of
these are useless because the two values for these variables consist
of an actual value and an NA. I am not sure what we can do with these
values, but let us try making a logistic regression model anyway,

```{r logisiticWith2ValVariables}

vals2numericVars <- with(num_vals_num_vars, label[num_vals == 2])
m.numeric2 <- glm(target ~ .,
                  data = train[, c(vals2numericVars, 'target')],
                  family='binomial'
                  )
tp.numeric2 <- predict(m.numeric2, newdata=train, type='response')
err.numeric2 <- sum( (tp.numeric2 > 0.5) != train$target, na.rm=TRUE)/nrow(train)

print("the logistic model with only numeric variables that take 2 values")
print(paste("training error :  ", round(err.numeric2,2)))
print(paste("compared to baseline: ", round(err.numeric2/baseline, 2)))
```
**No better than the null**


```{r logisiticWith3ValVariables}

vals3numericVars <- with(num_vals_num_vars, label[num_vals == 3])
m.numeric3 <- glm(target ~ .,
                  data = train.ecfixed[, c(vals3numericVars, 'target')],
                  family='binomial'
                  )
tp.numeric3 <- predict(m.numeric3, newdata=train.ecfixed, type='response')
err.numeric3 <- sum( (tp.numeric3 > 0.5) != train$target, na.rm=TRUE)/nrow(train)

print("the logistic model with only numeric variables that take 3 values")
print(paste("training error :  ", round(err.numeric3,2)))
print(paste("compared to baseline: ", round(err.numeric3/baseline, 2)))

```

Lets automate this process,

```{r logisticWithNvalsVars}

for(n in 2:max(var.entropies$distinct_vals)) {
    predictors <- Filter(
        function(p) {
            var.entropies[p, 'entropy'] > 0.1 &
                var.entropies[p, 'nacount'] < 1000
        },
        with(var.entropies, variable[distinct_vals == n &
                                         (datatype == 'numeric' |
                                              datatype == 'integer')])
        )
    print(paste("to try predictors with ", n , "distinct values"))
    print(paste("total number of such predictors ", length(predictors)))
    if(length(predictors) > 0) {
        m <- glm(target ~ .,
                 data = train[, c(predictors, 'target')],
                 family='binomial'
                 )
        tp <- predict(m, newdata=train, type='response')
        err <- sum( (tp > 0.5) != train$target, na.rm=TRUE)/nrow(train)
        print(paste(" logistic model with only num vars that take ", n, " values"))
        print(paste("total number of variables considered", length(predictors)))
        print(paste("training error :  ", round(err,2)))
        print(paste("compared to baseline: ", round(err/baseline, 2)))
        print("|||||||||||||||||||||||||||||||||||||||")
    }
}


```{r cors1}

cors1 <- cor(train[, num_vals_num_vars$label[127:136]], use="pairwise.complete.obs")

```
