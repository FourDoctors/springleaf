```{r setup, include=FALSE}

knitr::opts_chunk$set(cache=FALSE)
knitr::opts_chunk$set(echo=FALSE)
knitr::opts_chunk$set(include=TRUE)
knitr::opts_chunk$set(results="asis")
knitr::opts_chunk$set(fig.width=12)
knitr::opts_chunk$set(fig.height=14)
prjpath <- "~/work/learn/competitions/kaggle/springleaf/"
datapath <- paste(prjpath, "data/", sep="")
analpath <- paste(prjpath, "analysis/", sep="")
rcodepath <- paste(analpath, "Rcode/", sep="")
setwd(analpath)

```

```{r loadLibsAndSource, include=FALSE}

reqdpkgs <- c("lattice", "latticeExtra", "ggplot2", "reshape2",
              "plyr",  "lubridate", "Hmisc", "corrplot",
              "rpart", "rpart.plot", "rattle", "RWeka",
              "caret", "gbm", "randomForest")
lapply(reqdpkgs, library, character.only=TRUE)

```

Add sources here.
```{r sources}

source("../roc.R")

```

# Decision Trees to characterize the variables

We will use decision trees to characterize the predictor sets. For now
we assume that the datasets have been loaded in the environment, and
will fill in the details when we have time.

## Discrete encoded nunmeric variables

We can encode the discrete numeric variables (cluster X3 with entropy
smaller than 4 bits) with various levels of entropy-loss to reduce the
number of their distinct values. Which one is the best? We can answer
this question by running a decision tree to see if there is an effect
on the auc.

```{r encodedNums}

elosses <- c(1/8, 1/4, 1/2, 3/4)
fits.rpart.eloss.hc.num.X3 <- lapply(elosses,
                              function(max.loss) {
                                  data <- data.frame(
                                      lapply(train.model[, preds.num.X3.cats],
                                             function(xs) {
                                                 hxs <- huffman.encoded(xs,
                                                                        max.loss=max.loss)
                                                 print(paste("encoding for max-loss",
                                                             max.loss,
                                                             "entropy",
                                                             entropy(xs), "-->", entropy(hxs)
                                                             )
                                                       )
                                                 print(paste("encoding ",
                                                             "distinct",
                                                             length(unique(xs)), "-->",
                                                             length(unique(hxs))
                                                             )
                                                       )
                                                 print("----------------------------------")

                                                 hxs
                                             })
                                      )
                                  save(data, file=paste(
                                                 paste("../data/trainModelNumX3catsEloss",
                                                 max.loss, sep="-"), "Rdata", sep=".")
                                       )
                                  fit <- rpart(target ~ .,
                                               data = cbind(data,
                                                   data.frame(target=train$target)),
                                               control=rpart.control(cp=0.001,
                                                   depth=100, minsplit=10)
                                               )
                                  save(fit, file=paste( paste("../data/fitNumX3catsEloss",
                                                 max.loss, sep="-"), "Rdata", sep=".")
                                       )
                                  fit
                              })

auc.elosses <- sapply(fits.rpart.eloss.hc.num.X3,
                      function(fit) roc.auc(predict(fit), train$target)
                      )


print("effect of lossy encoding of discrete numerical variables")
print(data.frame(
    entropy.loss = elosses,
    auc.rpart = auc.elosses
    ))

```

We see that going from a loss of 0.125 bits to 0.750 reduced the auc
from by 0.04. The first thing to notice is that the auc itself is as
good as model.num.nz (that uses only the non-zero numerical variables
(preds.num.nz). This is probably because preds.num.X3.hc contain most
of the relevant information of preds.num.nz. We can see this by
looking at the decision trees for the fits.

```{r hcmodeltrees}

pdf(file="figures/treeEloss-0.125.pdf")
print(fancyRpartPlot(fits.rpart.eloss.hc.num.X3[[1]]))
dev.off()

pdf(file="figures/treeEloss-0.250.pdf")
print(fancyRpartPlot(fits.rpart.eloss.hc.num.X3[[2]]))
dev.off()

pdf(file="figures/treeEloss-0.500.pdf")
print(fancyRpartPlot(fits.rpart.eloss.hc.num.X3[[3]]))
dev.off()

pdf(file="figures/treeEloss-0.750.pdf")
print(fancyRpartPlot(fits.rpart.eloss.hc.num.X3[[4]]))
dev.off()
prp(fits.rpart.eloss.hc.num.X3[[1]], varlen=0)

```

We will use the 0.25 entropy loss.

## Data for models
We have noted in the prepare data code (in *prepareData.Rmd* or
*prepareData.R*) that the train and test data needs to be prepared
together. We have saved the results in the data/trainTest folder. We
will use this data for our models.

```{r loadsomedata}

load("../data/train_labeled.Rdata")
load("../data/test_labeled.Rdata")
N.train <- nrow(train)
N.test <- nrow(test)

```

```{r fitrpartnumnz}

if (!exists("train.and.test.num.nz")) {
    load("../data/trainTest/trainTestNumNZ.Rdata")
}

Y <- rep("NO", length=N.train)
Y[train$target == 1] <- "YES"
Y <- as.factor(Y)
train.num.nz <- train.and.test.num.nz[1:N.train,]
fit.rpart.nz <- rpart(target ~ .,
                      data = cbind(train.num.nz,
                          data.frame(target=Y)),
                      control=rpart.control(cp=0.001,
                          depth=100, minsplit=10)
                      )
print(roc.auc(predict(fit.rpart.num.nz), train$target))



```
The function _twoClassSummary_ from _caret_ does not seem to work,
returning NA for ROC. We will have to try our own,

```{r twoClassSummary}

ourTwoClassSummary <- function(data, lev=NULL, model=NULL) {
    rocObject <- roc(response=data$obs, predictor=data[, lev[1]])
    rocAUC <- rocObject$auc
    out <- c(rocAUC, sensitivity(data[, "pred"], data[, "obs"], lev[1]),
             specificity(data[, "pred"], data[, "obs"], lev[2]))
    names(out) <- c("ROC", "Sens", "Spec")
    out
}

```

Lets run a CV for rpart decision trees on non-num data,

```{r categoricalRpartCV}

if (!exists("train.and.test.non_num.treated")) {
    load("../data/trainTest/trainTestNonNumTreated.Rdata")
}
train.non_num <- train.and.test.non_num.treated[1:N.train, ]
test.non_num <- train.and.test.non_num.treated[(N.train + 1): (N.train + N.test),]

Y <- rep("NO", length=N.train)
Y[train$target == 1] <- "YES"
Y <- as.factor(Y)

fit.rpart.non_num <- rpart(target ~ .,
                           data = cbind(train.non_num,
                               data.frame(target=Y)),
                           control=rpart.control(cp=0.0008,
                               depth=100, minsplit=10)
                           )

print(roc.auc(predict(fit.rpart.non_num)[, 'YES'], train$target))


set.seed(100)
fit.rpart.non_num.cvtune.cp <- train(x = train.non_num,
                                     y = Y,
                                     method="rpart",
                                     tuneLength = 100,
                                     metric = "ROC",
                                     trControl = trainControl(
                                         summaryFunction = twoClassSummary,
                                         classProbs = TRUE,
                                         method = 'repeatedcv',
                                         repeats = 3)
                                     )

```

### A GBM model

```{r gbmtrainfunc}

fitgbm <- function(X,Y,
                   trControl = trainControl(
                       summaryFunction=twoClassSummary,
                       classProbs=TRUE,
                       method="repeatedcv",
                       number=10,
                       repeats=3),
                   grid = expand.grid (
                       interaction.depth=c(5,7,9),
                       n.trees=seq(400, 800, by=100),
                       shrinkage=c(0.01, 0.05, 0.1),
                       n.minobsinnode=10)
                   ) {
    gbmtune <- train(x = X, y = Y,
                     method="gbm",
                     metric="ROC",
                     tuneGrid=grid,
                     trControl=trControl)
    gbmtune

}

fitrpart <- function(X, Y,
                     trControl = trainControl(
                       summaryFunction=twoClassSummary,
                       classProbs=TRUE,
                       method="repeatedcv",
                       number=10,
                         repeats=3),
                     tunelength=10,
                     cp=TRUE) {

    method <- if (cp) "rpart" else "rpart2"
    rpartune <- train(x = X, y = Y,
                      method=method,
                      tuneLength = tunelength,
                      metric = "ROC",
                      trControl = trControl)
    rpartune
}


```


```{r gbmmodel1}

set.seed(100)
fitctrl <- trainControl(method="repeatedcv",
                        number=10,
                        repeats=2)
gbmGrid.0 <- expand.grid(interaction.depth=c(5,7,9),
                         n.trees=seq(400, 800, by=100),
                         shrinkage=c(0.01, 0.05, 0.1),
                         n.minobsinnode=10)
gbmtune.non_num.nz.g0 <- train(x=cbind(train.num.nz,
                                   train.non_num),
                               y=Y,
                               method='gbm',
                               metric='ROC',
                               tuneGrid=gbmGrid.0,
                               trControl=trainControl(
                                   summaryFunction=twoClassSummary,
                                   classProbs=TRUE,
                                   method="repeatedcv",
                                   number=10,
                                   repeats=3)
                               )

save(gbmGrid.0, file="../data/models/gbmTuneGrid0.Rdata")
save(gbmtune.non_num.nz.g0, file="../data/models/gbmtuneNonNumNZGrid0.Rdata")

rpart.non_num.nz.cvtune.cp <- train(x=cbind(train.num.nz,
                                        train.non_num),
                                    y=Y,
                                    method='rpart',
                                    trControl = trainControl(
                                        summaryFunction = twoClassSummary,
                                        classProbs = TRUE,
                                        method="repeatedcv",
                                        number=10,
                                        repeats=3)
                                    )

save(rpart.non_num.nz.cvtune.cp,
     file="../data/models/rpartNonNumNZcvtuneCP.Rdata")


```

## Catgegorical variables

There are not many categorical variables, so we can try a model with
all of them.

```{r fitcat}

fit.rpart.non_num.treated <- rpart(target ~ .,
                           data=cbind(train.model.non_num.treated,
                               data.frame(target=train$target)),
                           control=rpart.control(cp=0.001, depth=100, minsplit=10)
                                   )

print(roc.auc(predict(fit.rpart.non_num.treated), train$target))
save(fit.rpart.non_num.treated, file="../data/fits/rpart.non_num.treated")

```



Combine the categorical variables with the nz vars

```{r fitcatnz}

fit.rpart.non_num.nz <- rpart(target ~ .,
                              data=cbind(train.model.non_num.treated,
                                  train.model.num.nz,
                                  data.frame(target=train$target)
                                  ),
                              control=rpart.control(cp=0.001, depth=100, minsplit=10)
                              )
print(roc.auc(predict(fit.rpart.non_num.nz), train$target))

```

We can extract the important variables from our fit, and just use
those to see how well they predict

```{r fitimpvars}

vimp <- varImp(fit.rpart.non_num.nz)
vimp$variable <- row.names(vimp)
vimp <- vimp[order(vimp$Overall, decreasing=TRUE), ]
preds.non_num.nz.imp <- with(vimp, variable[Overall > 0])
print(paste("number of variables with position importance",
            length(preds.non_num.nz.imp))
      )

train.model.non_num.nz <- cbind(train.model.non_num.treated,
                                train.model.num.nz,)
control=rpart.control(cp=0.001, depth=100, minsplit=10)
fit.rpart.non_num.nz.imp <- rpart(target ~ .,
                                  data=cbind(
                                      train.model.non_num.nz[, preds.non_num.nz.imp],
                                      data.frame(target=train$target)
                                      ),
                                  control=control
                                  )

print(paste("when we use only the important variables we find an auc",
            roc.auc(predict(fit.rpart.non_num.nz.imp), train$target))
      )

```

Using only the important variables seems enough. We can use these and
add the ec variables.

```{r fitnonnumnzec}

primp.nz <- preds.non_num.nz.imp[grepl(pattern='numeric',
                                        x = preds.non_num.nz.imp)
                                  ]
primp.non <- preds.non_num.nz.imp[!grepl(pattern='numeric',
                                        x = preds.non_num.nz.imp)
                                  ]
fit.rpart.non.nz.ec <- rpart(target ~ .,
                             data=cbind(train.model.num.nz[, primp.nz],
                                 train.model.non_num.treated[, primp.non],
                                 train.model.num.ec),
                             control=control
                             )

```

We will use the variables' entropies and number NA to filter. Lets
save these in a dataframe

```{r varents}

var.entropies.X1 <- predictor.entropies(names(train.model.num.X1.ls),
                                        train.model.num.X1.ls, useNA=FALSE)
save(var.entropies.X1, file="../data/varEntropiesX1ls.Rdata")

var.entropies.X2 <- predictor.entropies(names(train.model.num.X2.ls),
                                        train.model.num.X2.ls, useNA=FALSE)
save(var.entropies.X2, file="../data/varEntropiesX2ls.Rdata")

var.entropies.X3.hc <- predictor.entropies(names(train.model.num.X3.cats.hc),
                                           train.model.num.X3.cats.hc, useNA=FALSE)
save(var.entropies.X3.hc, file="../data/varEntropiesX3hc.Rdata")

var.entropies.num.nz <- predictor.entropies(names(train.model.num.nz),
                                            train.model.num.nz, useNA=FALSE)
save(var.entropies.num.nz, file="../data/varEntropiesNumNZ.Rdata")

var.entropies.num.ec <- predictor.entropies(names(train.model.num.ec),
                                            train.model.num.ec, useNA=FALSE)
save(var.entropies.num.nz, file="../data/varEntropiesNumEC.Rdata")



var.entropies.non_num <- predictor.entropies(names(train.model.non_num.treated),
                                             train.model.non_num.treated, useNA=FALSE)
save(var.entropies.non_num, file="../data/varEntropiesNonNum.Rdata")

```

We will make models for NZ and EC data, and pick the predictors by
their importance for a full model,

```{r impvarsfunc}

importantVariables <- function(Xdata, Y,
                               control=rpart.control(depth=min(ncol(data), 100),
                                   minsplit=10,
                                   cp=0.001)
                               ) {
    fit <- rpart(target ~ .,
                 data=cbind(Xdata, data.frame(target=Y)),
                 control=control
                 )
    print(paste("fitted a model with auc",
                roc.auc(predict(fit), Y))
          )
    varimp <- varImp(fit)
    varimp$variable <- rownames(varimp)
    with(varimp, variable[Overall > 0])
}

```

```{r filterImpVars}

N <- nrow(train)
preds.fit.non_num <- importantVariables(
    train.model.non_num.treated[, with(var.entropies.non_num,
                                       variable[entropy > 0.5])],
    train$target
    )

preds.fit.num.X1 <- importantVariables(
    train.model.num.X1.ls[, with(var.entropies.X1,
                                 variable[entropy > 0.5 &
                                              nacount < N * 0.5]
                                 )
                         ],
    train$target
    )

preds.fit.num.X2 <- importantVariables(
    train.model.num.X2.ls[, with(var.entropies.X2,
                                 variable[entropy > 0.5 &
                                              nacount < N * 0.5]
                                 )
                          ],
    train$target
    )

preds.fit.num.X3.hc <- importantVariables(
    train.model.num.X3.cats.hc[, with(var.entropies.X3.hc,
                                 variable[entropy > 0.5 &
                                              nacount < N * 0.5]
                                 )
                          ],
    train$target
    )

preds.fit.num.ec <- importantVariables(
    train.model.num.ec[, with(var.entropies.num.ec,
                                 variable[entropy > 0.5 &
                                              nacount < N * 0.5]
                                 )
                          ],
    train$target
    )

preds.fit.num.nz <- importantVariables(
    train.model.num.nz[, with(var.entropies.num.nz,
                                 variable[entropy > 0.5 &
                                              nacount < N * 0.5]
                                 )
                          ],
    train$target
    )

```




```

Lets try a full model

```{r fullmodel}

preds.fit <- c(preds.fit.num.ec,
               preds.fit.num.nz,
               preds.fit.num.X1,
               preds.fit.num.X2,
               preds.fit.num.X3.hc,
               preds.fit.non_num
               )
train.fit <- cbind(train.model.non_num.treate,
                   train.model.num.treated)
train.fit <- train.fit[, preds.fit]
control <- rpart.control(depth=100, minsplit=10, cp=0.001)
fit.rpart.all <- rpart(target ~ .,
                       data=cbind(train.fit,
                           data.frame(target=train$target)
                                  ),
                       control=control
                       )
save(fit.rpart.all, file="../data/fitRpartAll")

```




```{r scratch}

data.test$date_7.year <- as.character(data.test$date_7.year)
data.test[data.test$date_7.year =="2007", 'date_7.year'] <- "2008"
data.test$date_7.year <- as.factor(data.test$date_7.year)
levels(data.test$date_7.year)


naed.unknownLevels <- function(data, p, unknown) {
    xs <- as.character(data[, p])
    xs[xs %in% unknown] <- NA
    as.factor(xs)
}

```
##Combine levels

We need to combine levels, train data does not have all the values that
the column might assume in test

```{r combinelevels}

combined.levels <- function(xs, ys) {
    cxs <- as.character(xs)
    cys <- as.character(ys)
    lxys <- levels(as.factor(c(cxs, cys)))
    list(factor(cxs, levels=lxys), factor(cys, levels=lxys))
}

```

## Submissions

### First mission

We will prepare data for our first submission,

```{r firstsubmission}

data.train <- cbind(train.model.num.X3.cats.hc[, preds.X3hc], train.model.non_num.treated)
data.test <- cbind(test.model.num.X3.cats.hc[, preds.X3hc], test.model.non_num.treated)

data.train$target <- train$target
fit1 <- rpart(target ~ ., data=data.train, control=control)

roc.auc(predict(fit1), train$target)

test.prediction <- data.frame(ID=test$ID, target=predict(fit1, newdata=data.test)



```


## Using pruned variables

We have created pruned variables, that we will now use for
predicting. We have also had problems with tuning GBMs. So we will
also train GBMs directly to debug.

```{r combineprunedvarsforprediction}

##assume the pp data to be loaded.
train.pp <- cbind(train.X1.nu.pp,
                  train.X1.nz.pp,
                  train.X1.ec.pp,
                  train.X2.pp,
                  train.X3.pp)

gbmodel <- gbm(target ~ .,
               data = cbind(train.pp,
                   data.frame(target=train$target) ),
               distribution = "bernoulli",
               interaction.depth = 9,
               n.trees = 100,
               shrinkage = 0.01,
               verbose = TRUE)

save(gbmodel, file="../data/models/gbmodel.pp.depth9.ntrees100.shrinkage01.Rdata")

```

The resuling model had a ROC of 0.7058721. We should compare this to
an rpart model.

```{r combinepprpart}

control = control=rpart.control(cp=0.001,
              depth=100, minsplit=10)
rpart.num.pp <- rpart(target ~ .,
                      data=cbind(train.pp,
                          data.frame(target=train$target) ),
                      control=control
                      )

save(rpart.num.pp, file="../data/models/rpart.num.pp.Rdata")

```








