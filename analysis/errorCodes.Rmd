```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=FALSE)
knitr::opts_chunk$set(echo=FALSE)
knitr::opts_chunk$set(include=TRUE)
knitr::opts_chunk$set(results="asis")
knitr::opts_chunk$set(fig.width=12)
knitr::opts_chunk$set(fig.height=14)
prjpath <- "~/work/learn/competitions/kaggle/springleaf/"
datapath <- paste(prjpath, "data/", sep="")
analpath <- paste(prjpath, "analysis/", sep="")
rcodepath <- paste(analpath, "Rcode/", sep="")
setwd(analpath)

```
Load the required libraries here. The user can know which packages
they will need in this one place, instead of running into uninstalled
packages later in the code.

```{r loadLibsAndSource, include=FALSE}

reqdpkgs <- c("lattice", "latticeExtra", "ggplot2", "reshape2",
              "plyr", "corrplot", "lubridate")
lapply(reqdpkgs, library, character.only=TRUE)

```

#Introduction

We have labelled the data in _label.Rmd_, which will be used
here. Our focus will be on numerical data, for which we have noticed
values which look as if they are some kind of error codes. Our goal is
to identify them using some heuristics, and replace them by NAs.

```{r dataload}

train <- read.csv(paste(datapath, 'train_labeled.csv', sep=""),
                  stringsAsFactor=FALSE)
test <- read.csv(paste(datapath, 'test_labeled.csv', sep=""),
                 stringsAsFactor=FALSE)

print(paste("number of columns in train", ncol(train)))
print(paste("number of columns in test", ncol(test)))

```
We have already dropped two columns before we saved the labeled data,
so the number of columns that we have read in is 1932, not 1934.


We will redo a few things like functions that we have already implemented in
_label.Rmd_,  probably better.

As we proceed with modeling this data, we have to filter out
columns. As we identify error-codes, we will find some columns that
will be eventually removed, and will save them in a list.

```{r variablesToRemove}

varsToRemove = data.frame(
    variable = c(),
    reason = c()
    )

```
We should add the name of the variable, and the reason to drop it, as
elements of this list.


# Types of the columns

```{r datatypes}

V <- ncol(train)
predictors <- names(train)[-V]
predictor_types <- sapply(predictors, function(p) class(train[, p]))
print("Data types in the data frame train")
print(table(predictor_types))

```
We will separate the columns by their data-type,

```{r colByDataType}

predictors_chr <- predictors[predictor_types == 'character']
predictors_num <- predictors[predictor_types == 'integer' |
                                 predictor_types == 'numeric']

```

# Whats in the numeric variables?

```{r numvals}

numeric_val_counts <- data.frame(list(
    label = as.character(predictors_num),
    num_vals = sapply(predictors_num,
        function(p) length(unique(train[, p]))
    )),
    stringsAsFactors=FALSE
)

histogram(~ log10(num_vals), data=numeric_val_counts)

```

## Numeric variables that have only 1 unique value!

One of the variables has only one distinct value. Which one?

```{r singlevalvar}

print(with(numeric_val_counts, label[num_vals == 1]))

varsToRemove <- rbind(varsToRemove,
                      data.frame(
                          list(
                              variable=with(numeric_val_counts,
                                  label[num_vals==1]),
                              reason='number of unique values is 1'
                              ),
                          stringsAsFactors=FALSE
                          )
                      )

print("the only value for numeric_792")
print(unique(train$numeric_792))
print("the only value for numeric_1373")
print(unique(train$numeric_1373))

```

## Distinct values for numeric variables.


We will write functions to count the number of distinct values assumed
by a variable.


```{r assumedvalsfew}

predictor.values.counts <- function(p, data=train){
    vals <- sort(unique(train[, p]), na.last=TRUE)
    tab <- data.frame(table(train[, p], useNA='ifany'))
    names(tab) <- c('value', 'count')
    tab$predictor <- p
    tab[, c("predictor", "value", "count")]
}

predictor.values <- function(vars, n, data=train) {
    mat <- cbind(vars,
                 do.call(rbind,
                         lapply(vars,
                                function(p) {
                                    sort(unique(train[, p]), na.last=TRUE)
                                })
                         )
                 )
    df <- data.frame(mat, stringsAsFactors=FALSE)
    names(df) <- c("variable", paste("value", 1:n, sep="."))
    df$entropy <- sapply(vars,
                         function(p) {
                             tab <- as.numeric(table(train[, p]))
                             prob <- tab/sum(tab)
                             -1 * sum(prob * log(prob))
                         })
    df$num.eff.vars <- exp(df$entropy)
    df
}

predictor.entropies <- function(vars, data=train, useNA='no') {
    df <- data.frame(
        list(
            variable = vars,
            entropy = sapply(vars, function(p) {
                                 tab <- as.numeric(table(train[,p]),
                                                   useNA = useNA)
                                 p <- tab/sum(tab)
                                 - sum(p * log2(p))
                             }),
            median = sapply(vars, function(p) {
                                median(train[,p], na.rm=TRUE)
                            }),
            max = sapply(vars, function(p) {
                             max(train[,p], na.rm=TRUE)
                         }),
            min = sapply(vars, function(p) {
                             min(train[, p], na.rm=TRUE)
                         }),
            distinct_vals = sapply(vars, function(p) {
                                       length(unique(train[,p], na.rm=TRUE))
                                   }),
            nacount = sapply(vars, function(p) {
                                 sum(is.na(train[,p]))
                             })
            ),
        stringsAsFactors=FALSE
        )

    df$num_eff_vals <- 2**(df$entropy)
    df <- df[order(df$num_eff_vals),]
    df
}

```

First we look at the entropy and the effective number of values for
each variable.

```{r varenteffval}

var.entropies <- predictor.entropies(numeric_val_counts$label)
print(head(var.entropies))

save(var.entropies, file=paste(datapath, "var.entropies"))

```

To see what we can do by investigating the distribution of values
assumed by a variable, we begin with  variables that take 3 distinct
values,

#### Three distinct valued numeric variables
```{r threedistinctnum1}

three.vals <- predictor.values(
    with(numeric_val_counts, label[num_vals==3]), 3
    )

three.vals.counts <- do.call(rbind,
                             lapply(three.vals$variable,
                                    predictor.values.counts)
                             )
print(three.vals.counts)

```

We can see that 27 of the 30 seemingly three valued variable are
really 2 valued. The third value is NA.


We can also view the correlations between the variables that assume 3
distinct values, to get an idea on trimming down on highly correlated
variable.

```{r threedistinctnum}

var.3val.cor <- cor(train[, with(var.entropies, variable[distinct_vals==3])],
                    use='pairwise.complete.obs')
var.3val.cor[is.na(var.3val.cor)] <- 0
corrplot(var.3val.cor, method='color')

```

And we can do the same thing for variables that assume four distinct
values.

```{r fourdistinctnum2}

var.4val.cor <- cor(train[, with(var.entropies, variable[distinct_vals==4])],
                    use='pairwise.complete.obs')
var.4val.cor[is.na(var.4val.cor)] <- 0
corrplot(var.4val.cor, method='color', tl.cex=0.5)

```

**A STRATEGY**


The correlation plot above shows us the way forward towards a
strategy. With about 2000 predictors, we will have to prune them. We
could use a sophisticated approach such as PCA, or partial least
squares. Or simply eyeball the correlation plots and choose one of the
highly correlated variables ( there are automated approaches in R to this,
which I will place in the appropriate section/file ).

In an example (not shown) , we can see pairs of
variables that are perfectly correlated (almost black boxes off the
diagonal). In fact, *numeric 1244, 1247, 1221, and 1220* seem to be
perfectly correlated. And being 3-valued they should be categorical
variables. We should expect a lot of such variables, even among
genuine numeric variables.

# Outlying values.

A numerical column with 1000 distinct values will contain outlying
values that look like 997 or 998. These could be codes for why the
data is missing. If considered as numerical values, these will distort
the distribution of that variable. We should consider declaring these
variables NA. But how do we identify these values?

```{r outlyingvalues}

outlying.values <- function(var, data=train, degree=2){
    uv <- train[,var]
    uv <- sort(uv[!is.na(uv)])
    uvneg <- unique(uv[ uv < 0])
    if(length(uvneg) == 1) negovs <- uvneg
    else negovs <- c()
    uv <- uv[ uv >= 0]
    luv <- log10(uv+1)
    if (length(luv) == 0) uvneg
    else {
        ovs <- unique(uv[ luv - mean(luv) > degree * sd(luv)])
        c(negovs, ovs)
    }
 }

```

```{r dpoutlyer}

dpoutlyers <- function(xs, data=train, offset=5, rarity = 0.001) {
    xs <- xs[!is.na(xs)]
    xsneg <- xs[xs < 0]
    xs <- xs[xs >= 0]
    olneg <- if (length(unique(xsneg)) == 1) unique(xsneg) else c()
    if(length(xs) == 0) return(olneg)
    txs <- table(xs)
    dps <- ceiling(log10(xs + offset))
    tdps <- table(dps)
    if(length(tdps) == 1) return(olneg)
    n <- max(dps)
    cdps <- rep(0, n)
    cdps[as.numeric(names(tdps))] <- as.numeric(tdps)
    if (length(cdps) <= 1) return(olneg)
    if ( (cdps[n-1] == 0) | (cdps[n]/cdps[n-1] < rarity)) {
        polps <- unique(xs[dps == n])
        polps <- polps[polps > 10]
        uxs <- unique(xs)
        olpos <- polps[ polps > 2*max(setdiff(uxs, polps))]
        c(olneg, olpos )
    }
    else {
        olneg
    }
}

```



With these function we can collect the outlying values for each
variable. We will use the dpoutlyer that uses the number of decimal
points in the number) to find the outlyers,

```{r outlyingvaluesevaluate}

outlying.values.list <- lapply(numeric_val_counts$label,
                               function(var) {
                                   print("outlyers for ")
                                   print(var)
                                   dpoutlyers(train[,var], rarity=0.001)
                                   }
                               )
names(outlying.values.list) <- numeric_val_counts$label

print("number of numeric variables with identified error-codes")
print(sum(sapply(outlying.values.list, function(xs) length(xs) > 0)))

```

### Setting outlying values to NA

Now that we have possible error-codes for each of the numerical
columns ( as the outliers), we can create a new dataframe that reports
these error-codes as NAs,

```{r markNAs}

save(outlying.values.list, file=paste(datapath, "outlyingValuesList.Rdata"))

train.ecfixed <- train
for(p in numeric_val_counts$label) {
    ov <- outlying.values.list[[p]]
    x <- train[,p]
    x[x %in% ov] <- NA
    train.ecfixed[,p] <- x
}

write.csv(train.ecfixed, file=paste(datapath, 'train_ecfixed.csv', sep=""))

test.ecfixed <- test
for(p in numeric_val_counts$label) {
    ob <- outlying.values.list[[p]]
    x <- test[,p]
    x[x %in% ov] <- NA
    test.ecfixed[,p] <- x
}

write.csv(test.ecfixed, file=paste(datapath, 'test_ecfixed.csv', sep=""))

```

*NOTE ON THE IDENTIFIED ERROR-CODES*
Most of the error-codes make sense, these are the largest 2 to 4 on a
given decimal point scale, for example 98 and 99. In other cases there
is a 165, or a list 32 40 23 24 30 22. These cases happen because they
are at least twice as large as the largest of the bulk (non-outlier)
value.

# How full are the columns?

Are there any columns without any NAs, and how many rows have all
their columns filled? We will look at the data in the error-code fixed
data,

```{r numnas}

nacount.rows <- rowSums(is.na(train.ecfixed))
nacount.cols <- data.frame(
    list(
        variable = names(train.ecfixed),
        count = sapply(names(train.ecfixed),
            function(p) sum(is.na(train.ecfixed[,p])))
        ),
    stringsAsFactors=FALSE
    )


predictors_allfilled <- with(nacount.cols, variable[count == 0])
predictors_num_allfilled <- intersect(predictors_allfilled, predictors_num)
var.entropies$nacount <- nacount.cols$count[var.entropies$variable]

```

```{r nonacolcor}

nonacol.cor <- cor(train.ecfixed[, predictors_num_allfilled])

```


## Correlations between variables
Some variables seem to contain the same information. For example
*numeric_192* is exactly the same as *numeric_193*,
```{r vars192and193}

print("correlation between numeric_192 and numeric_193")
print(with(train, cor(numeric_192, numeric_193, use="pairwise.complete.obs")))

```
We should use a map for keep track of which variables are identical!
```{r identicalvars}

identical_vars <- list()
identical_vars$numeric_192 <- c('numeric_193')

print("correlation between numeric_192 and numeric_193")
print(with(train, cor(numeric_192, numeric_193, use="pairwise.complete.obs")))

```
